\begin{tabular}{lrr}
\toprule
 & Mean ideal F1 score & Max ideal F1 score \\
Model &  &  \\
\midrule
o3 & 0.8809 & 0.8907 \\
gpt-4.5-preview & 0.8785 & 0.8844 \\
gpt-4.1-mini & 0.8716 & 0.8752 \\
Llama-4-Maverick-Instruct-Basic & 0.8643 & 0.8745 \\
gemini-2.0-pro-exp-02-05 & 0.8652 & 0.8740 \\
gemma-3-27b-it & 0.8551 & 0.8739 \\
DeepSeek-V3 & 0.8660 & 0.8725 \\
gemini-2.5-pro-exp-03-25 & 0.8672 & 0.8721 \\
Llama-3.1-405B-Instruct & 0.8636 & 0.8698 \\
o3-mini & 0.8632 & 0.8697 \\
Llama-4-Scout-Instruct-Basic & 0.8628 & 0.8663 \\
DeepSeek-R1 & 0.8549 & 0.8660 \\
gpt-4.1 & 0.8614 & 0.8658 \\
gemini-2.0-flash-lite & 0.8586 & 0.8614 \\
sabia-3 & 0.8149 & 0.8607 \\
QwQ-32B & 0.8438 & 0.8607 \\
gpt-4o-mini & 0.8566 & 0.8599 \\
Llama-3.1-70B-Instruct & 0.8566 & 0.8587 \\
Llama-3.3-70B-Instruct & 0.8562 & 0.8583 \\
sabiazinho-3 & 0.8452 & 0.8579 \\
o4-mini & 0.8525 & 0.8559 \\
o1 & 0.8489 & 0.8547 \\
gemma-2-27b-it & 0.8446 & 0.8539 \\
Llama-3-70B-Instruct & 0.8501 & 0.8531 \\
llama-3.2-1B-instruct-grpo-005-step-1000 & 0.8331 & 0.8490 \\
o1-mini & 0.8422 & 0.8469 \\
llama-3.2-1B-instruct-grpo-003 & 0.8267 & 0.8357 \\
gemini-2.0-flash & 0.8265 & 0.8329 \\
gpt-4.1-nano & 0.8086 & 0.8321 \\
gemma-3-4b-it & 0.8174 & 0.8263 \\
llama-3.2-1B-instruct-grpo-002 & 0.8074 & 0.8132 \\
gpt-4o & 0.7902 & 0.8128 \\
llama-3.2-1B-instruct-grpo & 0.7707 & 0.7940 \\
llama-3.2-1B-instruct-grpo-001 & 0.7717 & 0.7778 \\
Llama-3.2-3B-Instruct & 0.7341 & 0.7771 \\
llama-3.2-1B-instruct-grpo-004-step-0050 & 0.6878 & 0.7014 \\
DeepSeek-R1-Distill-Qwen-7B & 0.6687 & 0.6972 \\
DeepSeek-R1-Distill-Qwen-1.5B & 0.3710 & 0.5259 \\
gpt-4o-baseline & 0.4440 & 0.4440 \\
Llama-3.2-1B-Instruct & 0.0234 & 0.0494 \\
\bottomrule
\end{tabular}
