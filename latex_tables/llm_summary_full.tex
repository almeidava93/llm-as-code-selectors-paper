\begin{longtable}{lrllllrr}
\toprule
top_k & 10 & 20 & 50 & 100 & 200 & mean_f1_score & max_f1_score \\
model &  &  &  &  &  &  &  \\
\midrule
\endfirsthead
\toprule
top_k & 10 & 20 & 50 & 100 & 200 & mean_f1_score & max_f1_score \\
model &  &  &  &  &  &  &  \\
\midrule
\endhead
\midrule
\multicolumn{8}{r}{Continued on next page} \\
\midrule
\endfoot
\bottomrule
\endlastfoot
gpt-4.5-preview & 0.8564 & 0.8595 & 0.8671 & 0.8737 & 0.8710 & 0.8655 & 0.8737 \\
o3 & 0.8693 & 0.8568 & 0.8525 & 0.8734 &  & 0.8630 & 0.8734 \\
DeepSeek-V3 & 0.8477 & 0.8501 & 0.8587 & 0.8514 & 0.8644 & 0.8545 & 0.8644 \\
gemini-2.5-pro-exp-03-25 & 0.8405 & 0.8475 & 0.8506 & 0.8587 & 0.8594 & 0.8513 & 0.8594 \\
o3-mini & 0.8501 & 0.8450 & 0.8463 & 0.8579 & 0.8533 & 0.8505 & 0.8579 \\
Llama-4-Maverick-Instruct-Basic & 0.8309 & 0.8407 & 0.8453 & 0.8571 &  & 0.8435 & 0.8571 \\
gpt-4.1-mini & 0.8506 & 0.8541 & 0.8491 & 0.8568 &  & 0.8526 & 0.8568 \\
gemini-2.0-pro-exp-02-05 & 0.8401 & 0.8405 & 0.8541 & 0.8529 & 0.8568 & 0.8489 & 0.8568 \\
DeepSeek-R1 & 0.8336 & 0.8395 & 0.8420 & 0.8420 & 0.8556 & 0.8425 & 0.8556 \\
gpt-4.1 & 0.8482 & 0.8408 & 0.8509 & 0.8494 &  & 0.8473 & 0.8509 \\
Llama-3.1-405B-Instruct & 0.8498 & 0.8437 & 0.8455 & 0.8486 &  & 0.8469 & 0.8498 \\
QwQ-32B & 0.8122 & 0.8194 & 0.8354 & 0.8315 & 0.8468 & 0.8291 & 0.8468 \\
o4-mini & 0.8359 & 0.8372 & 0.8347 & 0.8459 &  & 0.8384 & 0.8459 \\
gemini-2.0-flash-lite & 0.8277 & 0.8371 & 0.8411 & 0.8453 &  & 0.8378 & 0.8453 \\
Llama-3.3-70B-Instruct & 0.8377 & 0.8408 & 0.8441 & 0.8449 &  & 0.8419 & 0.8449 \\
o1 & 0.8384 & 0.8285 & 0.8441 & 0.8398 &  & 0.8377 & 0.8441 \\
Llama-3.1-70B-Instruct & 0.8301 & 0.8349 & 0.8435 & 0.8381 &  & 0.8367 & 0.8435 \\
gemma-3-27b-it & 0.8421 & 0.8333 & 0.8322 & 0.8133 &  & 0.8302 & 0.8421 \\
gpt-4o-mini & 0.8399 & 0.8408 & 0.8397 & 0.8417 &  & 0.8405 & 0.8417 \\
sabia-3 & 0.7762 & 0.7677 & 0.7734 & 0.8400 &  & 0.7893 & 0.8400 \\
Llama-4-Scout-Instruct-Basic & 0.8345 & 0.8383 & 0.8391 & 0.8360 &  & 0.8370 & 0.8391 \\
o1-mini & 0.8280 & 0.8169 & 0.8338 & 0.8350 &  & 0.8284 & 0.8350 \\
sabiazinho-3 & 0.8116 & 0.8098 & 0.8313 & 0.8174 &  & 0.8175 & 0.8313 \\
Llama-3-70B-Instruct & 0.8171 & 0.8277 & 0.8281 & 0.8293 &  & 0.8255 & 0.8293 \\
gemma-2-27b-it & 0.8072 & 0.8224 & 0.8240 & 0.8229 &  & 0.8191 & 0.8240 \\
gemini-2.0-flash & 0.8073 & 0.8189 & 0.8079 & 0.8222 &  & 0.8141 & 0.8222 \\
gpt-4.1-nano & 0.8169 & 0.7908 & 0.7989 & 0.7825 &  & 0.7972 & 0.8169 \\
llama-3.2-1B-instruct-grpo-005-step-1000 & 0.8131 & 0.8163 & 0.8061 & 0.7820 &  & 0.8044 & 0.8163 \\
gpt-4o & 0.7651 & 0.7595 & 0.7988 & 0.8069 &  & 0.7826 & 0.8069 \\
llama-3.2-1B-instruct-grpo-003 & 0.8000 & 0.7983 & 0.7983 & 0.7933 &  & 0.7975 & 0.8000 \\
gemma-3-4b-it & 0.7938 & 0.7939 & 0.7955 & 0.7842 &  & 0.7919 & 0.7955 \\
llama-3.2-1B-instruct-grpo-002 & 0.7740 & 0.7699 & 0.7883 &  &  & 0.7774 & 0.7883 \\
llama-3.2-1B-instruct-grpo & 0.7622 & 0.7504 & 0.7208 & 0.7496 &  & 0.7458 & 0.7622 \\
Llama-3.2-3B-Instruct & 0.7316 & 0.7544 & 0.6453 & 0.7237 &  & 0.7137 & 0.7544 \\
llama-3.2-1B-instruct-grpo-001 & 0.7424 & 0.7496 & 0.7496 & 0.7431 &  & 0.7462 & 0.7496 \\
DeepSeek-R1-Distill-Qwen-7B & 0.6455 & 0.5925 & 0.6728 & 0.6790 &  & 0.6474 & 0.6790 \\
llama-3.2-1B-instruct-grpo-004-step-0050 & 0.6479 & 0.6562 & 0.6789 & 0.6667 &  & 0.6624 & 0.6789 \\
DeepSeek-R1-Distill-Qwen-1.5B & 0.5117 & 0.4630 & 0.3475 & 0.1308 &  & 0.3632 & 0.5117 \\
gpt-4o-baseline & 0.4269 &  &  &  &  & 0.4269 & 0.4269 \\
Llama-3.2-1B-Instruct & 0.0050 & 0.0000 & 0.0491 & 0.0389 &  & 0.0233 & 0.0491 \\
\end{longtable}
