\begin{tabular}{lrr}
\toprule
 & Mean F1 score & Max F1 score \\
Model &  &  \\
\midrule
gpt-4.5-preview & 0.8655 & 0.8737 \\
o3 & 0.8630 & 0.8734 \\
DeepSeek-V3 & 0.8545 & 0.8644 \\
gemini-2.5-pro-exp-03-25 & 0.8513 & 0.8594 \\
o3-mini & 0.8505 & 0.8579 \\
Llama-4-Maverick-Instruct-Basic & 0.8435 & 0.8571 \\
gpt-4.1-mini & 0.8526 & 0.8568 \\
gemini-2.0-pro-exp-02-05 & 0.8489 & 0.8568 \\
DeepSeek-R1 & 0.8425 & 0.8556 \\
gpt-4.1 & 0.8473 & 0.8509 \\
Llama-3.1-405B-Instruct & 0.8469 & 0.8498 \\
QwQ-32B & 0.8291 & 0.8468 \\
o4-mini & 0.8384 & 0.8459 \\
gemini-2.0-flash-lite & 0.8378 & 0.8453 \\
Llama-3.3-70B-Instruct & 0.8419 & 0.8449 \\
o1 & 0.8377 & 0.8441 \\
Llama-3.1-70B-Instruct & 0.8367 & 0.8435 \\
gemma-3-27b-it & 0.8302 & 0.8421 \\
gpt-4o-mini & 0.8405 & 0.8417 \\
sabia-3 & 0.7893 & 0.8400 \\
Llama-4-Scout-Instruct-Basic & 0.8370 & 0.8391 \\
o1-mini & 0.8284 & 0.8350 \\
sabiazinho-3 & 0.8175 & 0.8313 \\
Llama-3-70B-Instruct & 0.8255 & 0.8293 \\
gemma-2-27b-it & 0.8191 & 0.8240 \\
gemini-2.0-flash & 0.8141 & 0.8222 \\
gpt-4.1-nano & 0.7972 & 0.8169 \\
llama-3.2-1B-instruct-grpo-005-step-1000 & 0.8044 & 0.8163 \\
gpt-4o & 0.7826 & 0.8069 \\
llama-3.2-1B-instruct-grpo-003 & 0.7975 & 0.8000 \\
gemma-3-4b-it & 0.7919 & 0.7955 \\
llama-3.2-1B-instruct-grpo-002 & 0.7774 & 0.7883 \\
llama-3.2-1B-instruct-grpo & 0.7458 & 0.7622 \\
Llama-3.2-3B-Instruct & 0.7137 & 0.7544 \\
llama-3.2-1B-instruct-grpo-001 & 0.7462 & 0.7496 \\
DeepSeek-R1-Distill-Qwen-7B & 0.6474 & 0.6790 \\
llama-3.2-1B-instruct-grpo-004-step-0050 & 0.6624 & 0.6789 \\
DeepSeek-R1-Distill-Qwen-1.5B & 0.3632 & 0.5117 \\
Llama-3.2-1B-Instruct & 0.0233 & 0.0491 \\
\bottomrule
\end{tabular}
