Model,Mean F1 score,Max F1 score,Mean ideal F1 score,Max ideal F1 score
gpt-4.5-preview,0.8655131546037612,0.8736559139784947,0.8785333006003754,0.8843537414965987
o3,0.8629978871347317,0.8733509234828496,0.8809186585260371,0.8907103825136613
DeepSeek-V3,0.8544533883311292,0.8643617021276595,0.8659552411183672,0.87248322147651
gemini-2.5-pro-exp-03-25,0.8513465358817595,0.8594164456233422,0.8671728924533518,0.8721399730820997
o3-mini,0.850512971375942,0.8579161028416779,0.8632153821711814,0.869684499314129
Llama-4-Maverick-Instruct-Basic,0.843506202915923,0.8571428571428571,0.8643145558044537,0.874493927125506
gpt-4.1-mini,0.8526461883462049,0.856763925729443,0.8716055796398317,0.8751714677640603
gemini-2.0-pro-exp-02-05,0.8488874084971789,0.8567639257294428,0.8651513589949534,0.873972602739726
DeepSeek-R1,0.8425214875586697,0.855614973262032,0.8548685234974233,0.8660351826792962
gpt-4.1,0.8473481837385317,0.8509485094850948,0.8613785361700896,0.8658367911479945
Llama-3.1-405B-Instruct,0.8469099737828327,0.8497970230040596,0.8635531372288828,0.8698060941828255
QwQ-32B,0.8290538445831453,0.8467741935483871,0.8438124985299428,0.860655737704918
o4-mini,0.8384320152596744,0.8459459459459459,0.8524692238953014,0.8559322033898306
gemini-2.0-flash-lite,0.8378203285303251,0.8453333333333334,0.8585880962024758,0.8614130434782609
Llama-3.3-70B-Instruct,0.8418707706698768,0.8448753462603878,0.8562457045545054,0.8583450210378681
o1,0.8377022327062864,0.8441379310344828,0.8489039253030674,0.8547486033519553
Llama-3.1-70B-Instruct,0.8366736513140456,0.8435374149659863,0.8565547931142591,0.8587257617728532
gemma-3-27b-it,0.8302282644307837,0.8421052631578948,0.8550768589764439,0.8739495798319328
gpt-4o-mini,0.840528365794572,0.8416779431664412,0.8565681722802674,0.8599439775910365
sabia-3,0.7893201133144475,0.84,0.8148816641851769,0.8606557377049181
Llama-4-Scout-Instruct-Basic,0.8369733604151346,0.8391420911528151,0.8628052685344785,0.8662952646239556
o1-mini,0.8284049766459568,0.8349514563106797,0.8422218260279797,0.8469241773962803
sabiazinho-3,0.8175378970030271,0.8313090418353576,0.8452348482743719,0.8579387186629527
Llama-3-70B-Instruct,0.8255376537930557,0.8292682926829269,0.8501468718535853,0.8531468531468531
gemma-2-27b-it,0.8191289435714364,0.8240109140518418,0.8446278580226926,0.8539007092198581
gemini-2.0-flash,0.8140828766511452,0.8222222222222222,0.8265042739026252,0.8328611898016997
gpt-4.1-nano,0.7972496314382389,0.8169014084507041,0.8085878506432097,0.8321377331420372
llama-3.2-1B-instruct-grpo-005-step-1000,0.8043682123709798,0.8163265306122449,0.8330885772872157,0.849002849002849
gpt-4o,0.7825664643423339,0.8069164265129684,0.7901592546396619,0.8127721335268505
llama-3.2-1B-instruct-grpo-003,0.7975069156558189,0.8,0.8266770481251048,0.8357348703170029
gemma-3-4b-it,0.7918555171364265,0.795518207282913,0.8173914380376444,0.8262773722627738
llama-3.2-1B-instruct-grpo-002,0.7773994995753294,0.7883008356545961,0.8073562732583417,0.8132183908045978
llama-3.2-1B-instruct-grpo,0.745750946254879,0.7621776504297993,0.7706626348384582,0.7940298507462686
Llama-3.2-3B-Instruct,0.7137344894942379,0.7543859649122807,0.7341040311841519,0.7771084337349398
llama-3.2-1B-instruct-grpo-001,0.7461851100635735,0.7496382054992765,0.7716699043283336,0.7777777777777777
DeepSeek-R1-Distill-Qwen-7B,0.647426717473954,0.6789554531490016,0.6686793166114784,0.6971608832807571
llama-3.2-1B-instruct-grpo-004-step-0050,0.6624257682947841,0.6788990825688073,0.687759288701581,0.7014218009478672
DeepSeek-R1-Distill-Qwen-1.5B,0.3632476359111873,0.5117117117117117,0.37102421983024686,0.5259259259259259
Llama-3.2-1B-Instruct,0.023276818226164595,0.04914004914004913,0.023361222417149996,0.0493827160493827
